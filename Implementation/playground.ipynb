{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9855ac6e",
   "metadata": {},
   "source": [
    "Test Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c32cb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/users1/kashefnd/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "/projekte/semrel/WORK-AREA/Users/navid/mdenv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "nltk.download(\"brown\")\n",
    "from wordnet_interface import WordNetInterface\n",
    "from model import NThresholdModel, ContextualMaoModel,RandomBaseline,Models\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from embeddings import FasttextModel,WordAssociationEmbeddings, BertEmbeddings,Node2VecEmbeddingsCreator,Node2VecEmbeddings\n",
    "from data import Sentence, DataSet, Vectors\n",
    "from swow_interface import SWOWInterface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d065b8a7",
   "metadata": {},
   "source": [
    "Sentence Extraction Function for the two datasets used in the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd43d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit filepaths to the two datasets\n",
    "mohammad_filepath=\"\"\n",
    "knuples_filepath=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf088580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knuples_extractor(filepath, use_unsure):\n",
    "    sentences = []\n",
    "    fail_counter = 0\n",
    "    not_agree_counter=0\n",
    "    with open(filepath) as data:\n",
    "        i=0\n",
    "        for line in data:\n",
    "            i+=1\n",
    "            datapoint = line.split(\"\\t\")\n",
    "            if (datapoint[2] != \"unsure\" and datapoint[1] == datapoint[2]) or (\n",
    "                datapoint[2] == \"unsure\" and use_unsure\n",
    "            ):\n",
    "                try:\n",
    "                    if datapoint[2] == \"unsure\":\n",
    "                        value = 1\n",
    "                    elif datapoint[2] == \"literal\" and use_unsure:\n",
    "                        value = 2\n",
    "                    elif datapoint[2] == \"literal\":\n",
    "                        value = 1\n",
    "                    elif datapoint[2] == \"figurative\":\n",
    "                        value = 0\n",
    "                    else:\n",
    "                        print(f\"{datapoint[2]} is not a valid value\")\n",
    "                        raise ValueError(f\"{datapoint[2]} is not a valid value\")\n",
    "                    verb_sentence = Sentence(\n",
    "                        sentence=datapoint[3],\n",
    "                        target=datapoint[0].split()[0],\n",
    "                        value=value,\n",
    "                        phrase=datapoint[0],\n",
    "                        pos=\"v\",\n",
    "                    )\n",
    "                    noun_sentence = Sentence(\n",
    "                        sentence=datapoint[3],\n",
    "                        target=datapoint[0].split()[1],\n",
    "                        value=value,\n",
    "                        phrase=datapoint[0],\n",
    "                        pos=\"n\",\n",
    "                    )\n",
    "                    sentences += [verb_sentence, noun_sentence]\n",
    "                except ValueError as e:\n",
    "                    fail_counter += 1\n",
    "            else:\n",
    "                not_agree_counter+=1\n",
    "    print(f\"Ignored {fail_counter} bad sentences of {i} \")\n",
    "    print(f'Ignored {not_agree_counter} of {i} because of missing agreement to annotation')\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287e2f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mohammad_extractor(filepath, use_unsure):\n",
    "    sentences = []\n",
    "    fail_counter = 0\n",
    "    with open(filepath) as data:\n",
    "        data.readline()\n",
    "        i=0\n",
    "        for line in data:\n",
    "            i+=1\n",
    "            datapoint = line.split(\"\\t\")\n",
    "            if len(datapoint) == 5 and (float(datapoint[4]) >= 0.7 or use_unsure):\n",
    "                if float(datapoint[4]) < 0.7:\n",
    "                    value = 1\n",
    "                else:\n",
    "                    if datapoint[3] == \"literal\" and use_unsure:\n",
    "                        value = 2\n",
    "                    elif datapoint[3] == \"literal\":\n",
    "                        value = 1\n",
    "                    elif datapoint[3] == \"metaphorical\":\n",
    "                        value = 0\n",
    "                    else:\n",
    "                        print(f\"{datapoint[3]} is not a valid value\")\n",
    "                        fail_counter+=1\n",
    "                        raise ValueError(f\"{datapoint[3]} is not a valid value\")\n",
    "                try:\n",
    "                    # remove special tokens\n",
    "                    tokens = re.sub(r\"<.*?>\", \"\", datapoint[2])\n",
    "                    sentence = Sentence(\n",
    "                        sentence=tokens, target=datapoint[0], value=value, pos=\"v\"\n",
    "                    )\n",
    "                    sentences.append(sentence)\n",
    "                except ValueError:\n",
    "                    fail_counter += 1\n",
    "    print(f\"Ignored {fail_counter} of {i}\")\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe67272",
   "metadata": {},
   "source": [
    "Loading Interfaces, Embeddings and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1411003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating Interfaces\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'WordNetInterface' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mcreating Interfaces\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m wn= \u001b[43mWordNetInterface\u001b[49m()\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# edit the location of the complete response file of the SWOW Data, can also be loaded via parameter strength_file, which can be stored via method write_strengths_to_file\u001b[39;00m\n\u001b[32m      4\u001b[39m swow_r1_c2=SWOWInterface(number_of_responses=\u001b[32m1\u001b[39m,response_file=\u001b[33m\"\u001b[39m\u001b[33mSWOW-EN.complete.20180827.csv\u001b[39m\u001b[33m\"\u001b[39m,candidate_cap=\u001b[32m2\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'WordNetInterface' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"creating Interfaces\")\n",
    "wn= WordNetInterface()\n",
    "# edit the location of the complete response file of the SWOW Data, can also be loaded via parameter strength_file, which can be stored via method write_strengths_to_file\n",
    "swow_r1_c2=SWOWInterface(number_of_responses=1,response_file=\"SWOW-EN.complete.20180827.csv\",candidate_cap=2)\n",
    "swow_r1_ppmi=SWOWInterface(number_of_responses=1,response_file=\"SWOW-EN.complete.20180827.csv\",use_ppmi=True,candidate_cap=0)\n",
    "swow_r12_ppmi=SWOWInterface(number_of_responses=2,response_file=\"SWOW-EN.complete.20180827.csv\",use_ppmi=True,candidate_cap=0)\n",
    "swow_r123=SWOWInterface(number_of_responses=3,response_file=\"SWOW-EN.complete.20180827.csv\",candidate_cap=0)\n",
    "swow_r123_ppmi=SWOWInterface(number_of_responses=3,response_file=\"SWOW-EN.complete.20180827.csv\",candidate_cap=0,use_ppmi=True)\n",
    "wn = WordNetInterface()\n",
    "\n",
    "RANDOM_SEED=53 # Seed for creating training and test data, 53 for reproducing study results\n",
    "print(\"loading embeddings\")\n",
    "# Fasttext embeddings\n",
    "# ft_embeddings_wn = FasttextModel(load_file=fasttext_dir,fallback_source=wn)\n",
    "# BERT Embeddings\n",
    "contextual_embeddings=BertEmbeddings(layers=[9,10,11,12])\n",
    "# Matrix Factorization Embeddings\n",
    "# swow_embeddings_r12_ppmi=WordAssociationEmbeddings.create_graph_embeddings(swow=swow_r12_ppmi,index_file=\"cue_indices_manually_r12.tsv\",embedding_file=\"graph_embeddings/graph_embeddings_manually_ppmi_300_r12.npy\",use_only_cues=True,dimensions=300)\n",
    "matrix_embeddings_r12_ppmi=WordAssociationEmbeddings(swow=swow_r12_ppmi,index_file=\"cue_indices_manually_r12.tsv\",embedding_file=\"graph_embeddings/graph_embeddings_manually_ppmi_300_r12.npy\")\n",
    "# Node2vec Embeddings\n",
    "# takes a long time to train \n",
    "# n2v_ppmi_r123_creator=Node2VecEmbeddingsCreator(graph=swow_r123_ppmi,is_directed=False,p=0.5,q=0.5)\n",
    "# n2v_walks=n2v_ppmi_r123_creator.simulate_walks(num_walks=15,walk_length=75)\n",
    "# n2v_ppmi_r123_creator.create_embeddings(save_file=\"graph_embeddings/n2v_r123_ppmi_sg.kv\",walks=n2v_walks,dimensions=256,window_size=12,sg=True)\n",
    "#n2v_ppmi_r123=Node2VecEmbeddings(loadf\"graph_embeddings/n2v_r123_ppmi_sg.kv\"),swow=swow_r123_ppmi)\n",
    "\n",
    "print(\"loading datasets\")\n",
    "knuples_data = DataSet(\n",
    "    filepath=knuples_filepath, extraction_function=knuples_extractor, use_unsure=False, test_seed=RANDOM_SEED,test_split_size=0.2\n",
    "    )\n",
    "# knuples_data_unsure=DataSet(\n",
    "# filepath=knuples_dataset,extraction_function=knuples_extractor,use_unsure=True,test_seed=RANDOM_SEED,test_split_size=0.2\n",
    "    # )\n",
    "# mohammad_data_unsure=DataSet(\n",
    "#     filepath=mohammad_dataset,extraction_function=mohammad_extractor,use_unsure=True,test_seed=RANDOM_SEED,test_split_size=0.2\n",
    "#     )\n",
    "mohammad_data = DataSet(\n",
    "    filepath=mohammad_filepath,\n",
    "    extraction_function=mohammad_extractor,\n",
    "    use_unsure=False,test_seed=RANDOM_SEED,test_split_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0878b467",
   "metadata": {},
   "source": [
    "The five best models from our study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45951f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_context=ContextualMaoModel(data=mohammad_data,candidate_source=wn,mean_multi_word=True,fit_embeddings=contextual_embeddings,score_embeddings=contextual_embeddings,use_context_vec=True,apply_candidate_weight=False,restrict_pos=[\"v\"],num_classes=2)\n",
    "swow_candidates_target=ContextualMaoModel(data=mohammad_data,candidate_source=swow_r123,mean_multi_word=True,fit_embeddings=contextual_embeddings,score_embeddings=contextual_embeddings,use_context_vec=False,apply_candidate_weight=False,restrict_pos=[\"v\"],num_classes=2)\n",
    "swow_candidates_context=ContextualMaoModel(data=mohammad_data,candidate_source=swow_r1_c2,mean_multi_word=True,fit_embeddings=contextual_embeddings,score_embeddings=contextual_embeddings,use_context_vec=True,apply_candidate_weight=True,restrict_pos=[\"v\"],num_classes=2)\n",
    "swow_embeddings_matrix=NThresholdModel(data=mohammad_data,candidate_source=wn,mean_multi_word=False,fit_embeddings=matrix_embeddings_r12_ppmi,score_embeddings=matrix_embeddings_r12_ppmi,use_output_vec=False,apply_candidate_weight=False,restrict_pos=[\"v\"],num_classes=2)\n",
    "# Node2vec takes a long time to train, you can use Matrix embeddings instead\n",
    "swow_model_n2v=NThresholdModel(data=mohammad_data,candidate_source=swow_r1_c2,mean_multi_word=False,fit_embeddings=matrix_embeddings_r12_ppmi,score_embeddings=matrix_embeddings_r12_ppmi,use_output_vec=False,apply_candidate_weight=False,restrict_pos=[\"v\"],num_classes=2)\n",
    "#swow_model_n2v=NThresholdModel(data=mohammad_data,candidate_source=swow_r1_c2,mean_multi_word=False,fit_embeddings=n2v_ppmi_r123,score_embeddings=n2v_ppmi_r123,use_output_vec=False,apply_candidate_weight=False,restrict_pos=[\"v\"],num_classes=2)\n",
    "random_baseline=RandomBaseline(data=mohammad_data,candidate_source=wn,score_embeddings=matrix_embeddings_r12_ppmi,restrict_pos=None,num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e222e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "models:list[NThresholdModel]=[baseline_context,swow_candidates_target,swow_candidates_context,swow_embeddings_matrix,swow_model_n2v,random_baseline]\n",
    "filenames=[\"Baseline Context\",\"SWOW Candidates Target\",\"SWOW Candidates Context\",\"test SWOW Embeddings Matrix\",\"test SWOW Both\",\"Random Baseline\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6255d5f",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e699bbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Context\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1014/1014 [03:27<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignored 0 sentences of 1014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 746/746 [00:00<00:00, 1631.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Thresholds: [tensor(0.7301, device='cuda:0')]\n",
      "Best score:0.525204946607464\n",
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 253/253 [00:31<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 20.  54.]\n",
      " [ 25. 154.]]\n",
      "ignored 0 sentences of 253\n",
      "drawing distributions\n",
      "SWOW Candidates Target\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1014/1014 [01:58<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignored 0 sentences of 1014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 608/608 [00:00<00:00, 1657.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Thresholds: [tensor(0.7760, device='cuda:0')]\n",
      "Best score:0.5188194444444445\n",
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 253/253 [00:14<00:00, 17.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.  29.]\n",
      " [ 33. 179.]]\n",
      "ignored 0 sentences of 253\n",
      "drawing distributions\n",
      "SWOW Candidates Context\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1014/1014 [02:09<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignored 0 sentences of 1014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:00<00:00, 1626.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Thresholds: [tensor(0.5412, device='cuda:0')]\n",
      "Best score:0.5485315623282631\n",
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 253/253 [00:29<00:00,  8.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.  26.]\n",
      " [ 34. 182.]]\n",
      "ignored 0 sentences of 253\n",
      "drawing distributions\n",
      "test SWOW Embeddings Matrix\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1014/1014 [00:00<00:00, 2070.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignored 29 sentences of 1014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 631/631 [00:00<00:00, 26528.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Thresholds: [0.08510261864089072]\n",
      "Best score:0.6165800749433372\n",
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 253/253 [00:00<00:00, 2202.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 20.  58.]\n",
      " [ 25. 146.]]\n",
      "ignored 4 sentences of 253\n",
      "drawing distributions\n",
      "test SWOW Both\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1014/1014 [00:00<00:00, 2487.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignored 14 sentences of 1014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 525/525 [00:00<00:00, 25919.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Thresholds: [0.2798436058397144]\n",
      "Best score:0.5342338146250583\n",
      "evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 253/253 [00:00<00:00, 2531.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 18.  55.]\n",
      " [ 27. 151.]]\n",
      "ignored 2 sentences of 253\n",
      "drawing distributions\n",
      "drawing ROC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 253/253 [00:26<00:00,  9.46it/s]\n",
      "100%|██████████| 253/253 [00:13<00:00, 19.13it/s]\n",
      "100%|██████████| 253/253 [00:28<00:00,  8.91it/s]\n",
      "100%|██████████| 253/253 [00:00<00:00, 1845.02it/s]\n",
      "100%|██████████| 253/253 [00:00<00:00, 2344.35it/s]\n",
      "100%|██████████| 253/253 [00:00<00:00, 8018.91it/s]\n"
     ]
    }
   ],
   "source": [
    "for model, name in zip(models, filenames):\n",
    "    if name!= \"Random Baseline\":\n",
    "        print(name)\n",
    "        print(\"training\")\n",
    "        model.train_thresholds(metrics=[\"macro_f_1\"],data=model.train_dev_data,by_pos=[\"v\"],by_phrase=False)\n",
    "        print(\"evaluating\")\n",
    "        model.evaluate(data=model.test_data,save_file=\"model_results/\"+name+\".txt\",by_pos=[\"v\"],by_phrase=False)\n",
    "        print(\"drawing distributions\")\n",
    "        model.draw_distribution_per_class(\n",
    "            save_file=\"model_distributions/\" + name + \".png\",\n",
    "            title=name,\n",
    "            data=model.test_data,\n",
    "            by_pos=[\"v\"],\n",
    "            labels=[\"metaphorical\",\"literal\"],\n",
    "        )\n",
    "print(\"drawing ROC\")\n",
    "Models.get_recall_curve(\n",
    "    data=models[0].test_data,\n",
    "    save_file=\"recall_curves/\" + \"_\".join(filenames) + \".png\",\n",
    "    models=models,\n",
    "    graph_labels=filenames,\n",
    "    by_pos=[\"v\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d6bec5",
   "metadata": {},
   "source": [
    "Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72269716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the wings of the birds clapped loudly clap 1\n",
      "beat tensor(0.7303, device='cuda:0') 1\n",
      "{'applaud', 'clap', 'put', 'gesticulate', 'hit', 'beat', 'spat'}\n"
     ]
    }
   ],
   "source": [
    "model=baseline_context\n",
    "i=50\n",
    "sentence=model.train_dev_data[i]\n",
    "print(sentence.sentence,sentence.target,sentence.value)\n",
    "print(model.best_fit(sentence),model.get_compare_value(sentence),model.predict(sentence))\n",
    "print(model.candidate_source.get_candidate_set(sentence.target,pos=[\"v\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
